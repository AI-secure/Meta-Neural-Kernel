{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Omniglot: Meta Neural Kernel vs. MAML & iMAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta_cntk_utils import *\n",
    "from maml_utils import *\n",
    "from tqdm.notebook import trange\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}\n",
    "n_characters = 20 # number of characters (i.e., classes) in the training dataset\n",
    "n_way=  5 \n",
    "n_shot = 1\n",
    "gpu_id = 3\n",
    "seed = 3\n",
    "# random_cnn determines if we use a random or \n",
    "# trained CNN as the feature extractor\n",
    "# if False, then we train a CNN over the training dataset\n",
    "# in the supervised learning way. After that, we take its hidden\n",
    "# layers as the feature extractor.\n",
    "random_cnn = False\n",
    "label_encoding_dim = 250 # last hidden size of the CNN\n",
    "\n",
    "# If pre-train a CNN on the training data,\n",
    "# pretrain_epochs sets the number of epochs\n",
    "# for the pre-training\n",
    "pretrain_epochs = 50\n",
    "# Certainly, we can resample from the\n",
    "# n_way classes to obtain more training tasks\n",
    "train_data_enlarge_ratio =15\n",
    "maml_epochs = 200\n",
    "kernel = 'CNTK' # Take Convlutional NTK as the base kernel\n",
    "# For simplicity, we only consider \n",
    "# cases s.t. n_characters % n_way == 0\n",
    "assert n_characters % n_way == 0 \n",
    "# Each task consists of n_way classes \n",
    "# So the least number of tasks is\n",
    "# n_characters//n_way.\n",
    "n_task = n_characters//n_way\n",
    "\n",
    "# Determine the dimention of label encodings\n",
    "\n",
    "#Random CNN: emb-dim = 1000 is better than 500 or 2000\n",
    "\n",
    "batch_norm = True # batch_norm for the feature extractor\n",
    "dropout = 0 # Dropout for the feature extractor\n",
    "pretrain_batch_size = 16\n",
    "weight_decay = 0\n",
    "\n",
    "\n",
    "# number of channels for MAML CNN\n",
    "n_channel_maml = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(gpu_id)\n",
    "device = torch.device('cuda')\n",
    "t0 = time()\n",
    "# Load dataset\n",
    "dataset = load_dataset(n_task,True,seed)\n",
    "\n",
    "# train_set = get_train_data(dataset)\n",
    "\n",
    "train_set,test_set = get_train_data(dataset,n_test_per_class=1)\n",
    "\n",
    "# Construct a randomly initialized CNN as the feature extractor.\n",
    "net = build_CNN(train_set['n_class'], device, n_channel=label_encoding_dim,batch_norm=batch_norm,dropout=dropout)\n",
    "\n",
    "if not random_cnn and pretrain_epochs > 0:\n",
    "    # Train a CNN on training data by supervised learning, in order\n",
    "    # to obtain a better feature extractor than a random CNN when\n",
    "    # training data is relatively large. As the training data are of \n",
    "    # small-size, the supervised training leads to overfitted CNN,\n",
    "    # which is a worse feature extractor than a random CNN.\n",
    "    print('Pre-training the feature extractor')\n",
    "    net,test_accs,test_losses = pretrain(net,train_set,test_set,device,seed=seed,epochs=pretrain_epochs,\n",
    "                                         weight_decay=weight_decay,batch_size=pretrain_batch_size)\n",
    "\n",
    "encode_labels(dataset,net,device)\n",
    "\n",
    "# Given n_way*n_task classes of samples, we can us resampling to obtain many tasks that consists of n_way distinct classes\n",
    "# of samples. The following is the resampling procedure.\n",
    "orig_dataset = deepcopy(dataset)\n",
    "if train_data_enlarge_ratio > 1:\n",
    "    augment_train_data(dataset,enlarge_ratio=train_data_enlarge_ratio,n_way=n_way,n_shot=n_shot,seed=seed)\n",
    "\n",
    "# get_embeddings_from_PCA(dataset,PCA_method=PCA_method,n_components=n_components)\n",
    "preprocess_label_embeddings(dataset)\n",
    "load_precomputed_base_kernels(dataset,kernel=kernel)\n",
    "print(f'Preprocessing takes {round(time()-t0,1)}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment on Meta Neural Kernels\n",
    "Note: MetaCNTK = Meta Neural Kernel with CNTK as base kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaCNTK = build_MetaCNTK(dataset,normalize_NTK=True,normalize_metaNTK=True)\n",
    "metaCNTK_test_acc, metaCNTK_pred, metaCNTK_loss = test_MetaCNTK(dataset,metaCNTK)\n",
    "metaCNTK_test_acc *= 100 # Numerical -> percentage\n",
    "accs['mnk_accs'] = metaCNTK_test_acc\n",
    "print(\"MetaCNTK Accuracy:\",metaCNTK_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment on MAML\n",
    "The core code and hyperparameters is adopted from `higher`, a pytorch package: https://github.com/facebookresearch/higher/blob/master/examples/maml-omniglot.py\n",
    "### Prepare dataset and build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: train (20, 20, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_label_encodings = False # If True, use encoded labels and l2 loss for MAML\n",
    "l2_loss = use_label_encodings\n",
    "preprocess_label_embeddings(orig_dataset)\n",
    "tasks = vars(orig_dataset)\n",
    "\n",
    "X = np.concatenate([tasks['X_qry'],tasks['X_spt']],axis=1)\n",
    "Y = np.concatenate([tasks['Y_qry'],tasks['Y_spt']],axis=1)\n",
    "Y_emb = np.concatenate([tasks['Y_qry_emb'],tasks['Y_spt_emb']],axis=1)\n",
    "new_X =[]\n",
    "new_Y_emb = []\n",
    "for x,y,y_emb in zip(X,Y,Y_emb):\n",
    "    idxes = np.argsort(y).reshape(n_way,-1)\n",
    "    for i in range(idxes.shape[0]):\n",
    "        new_X.append([])\n",
    "        new_Y_emb.append([])\n",
    "        for j in range(idxes.shape[1]):\n",
    "            idx = idxes[i,j]\n",
    "            new_X[-1].append(x[idx])\n",
    "            new_Y_emb[-1].append(y_emb[idx])\n",
    "x_train = remove_padding(np.array(new_X))\n",
    "y_train = np.array(new_Y_emb) if use_label_encodings else None\n",
    "if l2_loss:\n",
    "    test_tasks = remove_padding(tasks['test_X_spt']), tasks['test_Y_spt_emb'],remove_padding(tasks['test_X_qry']), tasks['test_Y_qry_emb']\n",
    "else:\n",
    "    test_tasks = remove_padding(tasks['test_X_spt']), tasks['test_Y_spt'],remove_padding(tasks['test_X_qry']), tasks['test_Y_qry']\n",
    "\n",
    "from support.omniglot_loaders_original import OmniglotNShot\n",
    "n_channel = n_channel_maml\n",
    "batchsz = 32 if n_channel <= 1024 else 8\n",
    "\n",
    "db = OmniglotNShot(root=None,\n",
    "    batchsz=batchsz,\n",
    "    n_way=5,\n",
    "    k_shot=1,\n",
    "    k_query=19,\n",
    "    imgsz=28,\n",
    "    device=device,\n",
    "    n_train_tasks=None,\n",
    "    given_x=True,\n",
    "    x_train=x_train,\n",
    "    x_test=None,\n",
    "    y_train = y_train,\n",
    ")\n",
    "n_out = label_encoding_dim if use_label_encodings else n_way\n",
    "net, meta_opt = build_MAML_model(n_out,device,lr=1e-3,n_channel=n_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "t = trange(maml_epochs,desc='MAML Training')\n",
    "maml_test_accs = []\n",
    "for epoch in t:\n",
    "    train_acc = train_MAML(db, net, device, meta_opt, epoch, log,verbose=False,l2_loss = l2_loss)\n",
    "#     t.set_postfix()\n",
    "    if epoch % 5 == 0:\n",
    "        test_acc=test_MAML(db, net, device, epoch, log,test_tasks,verbose=False,l2_loss=l2_loss,dataset=dataset)\n",
    "        maml_test_accs.append(test_acc)\n",
    "        if use_label_encodings:\n",
    "            # We did not implement the function to calculate training accuracy in the l2 loss case.\n",
    "            t.set_postfix(test_acc=test_acc,max_test_acc=np.max(maml_test_accs))\n",
    "        else:\n",
    "            t.set_postfix(train_acc=train_acc,test_acc=test_acc,max_test_acc=np.max(maml_test_accs))\n",
    "accs['maml_accs'] =np.max(maml_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment on implicit MAML (iMAML)\n",
    "The code is adopted from https://github.com/prolearner/hypertorch/tree/master/hypergrad, along with default hyperparameters (copied below).\n",
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imaml_utils import *\n",
    "hg_mode = 'CG'\n",
    "inner_log_interval = None\n",
    "inner_log_interval_test = None\n",
    "ways = n_way\n",
    "shots = n_shot\n",
    "test_shots=20-n_shot\n",
    "batch_size = 16\n",
    "n_channels = 64\n",
    "reg_param = 2  # reg_param = 2\n",
    "T, K = 16, 5  # T, K = 16, 5\n",
    "T_test = T\n",
    "inner_lr = .1\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "# the following are for reproducibility on GPU, see https://pytorch.org/docs/master/notes/randomness.html\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "meta_model = get_cnn_omniglot(n_channels, ways).to(device)\n",
    "\n",
    "outer_opt = torch.optim.Adam(params=meta_model.parameters())\n",
    "inner_opt_class = hg.GradientDescent\n",
    "inner_opt_kwargs = {'step_size': inner_lr}\n",
    "\n",
    "def get_inner_opt(train_loss):\n",
    "    return inner_opt_class(train_loss, **inner_opt_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 5\n",
    "log_interval = 5\n",
    "test_accs = []\n",
    "t = trange(maml_epochs,desc='i-MAML Epoch')\n",
    "for k in t:\n",
    "    train_imaml(meta_model,db,reg_param,hg_mode,K,T,outer_opt,inner_log_interval)\n",
    "    if k % eval_interval == 0:\n",
    "        test_losses, test_acc = test_imaml(test_tasks, meta_model, T_test, get_inner_opt, reg_param, log_interval=None)\n",
    "        test_acc = np.mean(test_acc)*100\n",
    "        test_accs.append(test_acc)\n",
    "        t.set_postfix(test_acc=test_acc,max_test_acc=np.max(test_accs))\n",
    "accs['i-maml_accs'] =np.max(test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy for MNK, MAML and iMAML\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
